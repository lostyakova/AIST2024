{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f175ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87296bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from statistics import mode\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0516f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_labels(golden_cut: list):\n",
    "    \"\"\"Parameters:\n",
    "        golden_cut (list): label list from golden annotation data\n",
    "    Returns:\n",
    "        short_labels (list): cut (short) labels from the original label list\"\"\"\n",
    "    short_labels = []\n",
    "    for i in range(len(golden_cut)):\n",
    "        if 'Open' in golden_cut[i]:\n",
    "            if 'Initiate' in golden_cut[i]:\n",
    "                short_labels.append(re.sub('Initiate.','', golden_cut[i]))\n",
    "\n",
    "            if len(golden_cut[i].split('.')) == 3:\n",
    "                short_labels.append('.'.join(golden_cut[i].split('.')[:-1]))\n",
    "            else:\n",
    "                short_labels.append(golden_cut[i])\n",
    "        elif \"Prolong\" in golden_cut[i] or \"Develop\" in golden_cut[i]:\n",
    "            short_labels.append('.'.join(golden_cut[i].split('.')[:-1]))\n",
    "        elif \"Track\" in golden_cut[i]:\n",
    "            short_labels.append('.'.join(golden_cut[i].split('.')[:-1]))\n",
    "        elif \"Reply\" in golden_cut[i]:\n",
    "            if \"Accept\" not in golden_cut[i]:\n",
    "                short_labels.append('.'.join(golden_cut[i].split('.')[:-1]))\n",
    "            else:\n",
    "                short_labels.append(golden_cut[i])\n",
    "        elif \"Challenge\" in golden_cut[i]:\n",
    "            short_labels.append('.'.join(golden_cut[i].split('.')[:-1]))\n",
    "        elif \"Confront.Response\" in golden_cut[i]:\n",
    "            short_labels.append('.'.join(golden_cut[i].split('.')[:-1]))\n",
    "        else:\n",
    "            short_labels.append(golden_cut[i])\n",
    "\n",
    "    return short_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108923e1",
   "metadata": {},
   "source": [
    "# Prepare clean crowdsource data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b80ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdsource = pd.read_csv('assignments.tsv', sep='\\t')\n",
    "# drop na values\n",
    "crowdsource = crowdsource.dropna(subset=['OUTPUT:labels'])\n",
    "# making dataset more useable\n",
    "crowdsource = crowdsource[crowdsource.columns[[1, 4, 6]]]\n",
    "crowdsource = crowdsource.rename(columns={'INPUT:dialog_id': 'dialog_id', \n",
    "                                          'OUTPUT:labels': 'label', \n",
    "                                          'INPUT:utterances': 'utterance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c59ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split label string\n",
    "crowdsource.label = crowdsource.label.apply(lambda x: x.replace('&nbsp;', '').split(','))\n",
    "# split html utterances\n",
    "crowdsource.utterance = crowdsource.utterance.apply(\n",
    "    lambda x: re.split(r',?<span class=\"speaker_\\d\">speaker_\\d</span>: ', x.replace('�', \"'\"))[1:]\n",
    ")\n",
    "crowdsource['short_label'] = crowdsource.label.apply(cut_labels)\n",
    "\n",
    "# split utterances and labels into rows\n",
    "crowdsource = crowdsource.explode(['utterance', 'label', 'short_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a68c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>label</th>\n",
       "      <th>short_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Can you do push-ups?</td>\n",
       "      <td>Open.Demand.Fact</td>\n",
       "      <td>Open.Demand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Of course I can.</td>\n",
       "      <td>React.Respond.Support.Reply.Agree</td>\n",
       "      <td>React.Respond.Support.Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It's a piece of cake!</td>\n",
       "      <td>Sustain.Continue.Prolong.Extend</td>\n",
       "      <td>Sustain.Continue.Prolong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Believe it or not.</td>\n",
       "      <td>Sustain.Continue.Prolong.Enhance</td>\n",
       "      <td>Sustain.Continue.Prolong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I can do 30 push-ups a minute.</td>\n",
       "      <td>Sustain.Continue.Prolong.Enhance</td>\n",
       "      <td>Sustain.Continue.Prolong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialog_id                       utterance  \\\n",
       "0        1.0            Can you do push-ups?   \n",
       "0        1.0                Of course I can.   \n",
       "0        1.0           It's a piece of cake!   \n",
       "0        1.0              Believe it or not.   \n",
       "0        1.0  I can do 30 push-ups a minute.   \n",
       "\n",
       "                               label                  short_label  \n",
       "0                   Open.Demand.Fact                  Open.Demand  \n",
       "0  React.Respond.Support.Reply.Agree  React.Respond.Support.Reply  \n",
       "0    Sustain.Continue.Prolong.Extend     Sustain.Continue.Prolong  \n",
       "0   Sustain.Continue.Prolong.Enhance     Sustain.Continue.Prolong  \n",
       "0   Sustain.Continue.Prolong.Enhance     Sustain.Continue.Prolong  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowdsource.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5f834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdsource.to_csv('assignments_2col.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290adec2-e1aa-4cc8-ae9b-9212aa127fba",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a814ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdsource = pd.read_csv('assignments_2col.csv')\n",
    "gold_dialogs = pd.read_csv(\"gold_standard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910da174",
   "metadata": {},
   "source": [
    "# Change text labels to numeric\n",
    "\n",
    "1. Prepare `gold_standard.csv`:\n",
    "    - str labels -> list of labels\n",
    "    - replace Initiate classes with the common ones\n",
    "2. Get long and short labels from gold standard and create `label2id` dictionary\n",
    "\n",
    "*There are 32 classes for lomg labels and 16 classes for short labels*\n",
    "\n",
    "3. Replace class labels with numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053dc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = lambda x: re.findall(r'\\'(\\S+)\\'', x)\n",
    "gold_dialogs.gold_answers = gold_dialogs.gold_answers.apply(parse)\n",
    "gold_dialogs.possible_answers = gold_dialogs.possible_answers.apply(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444d7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "change = {\n",
    "    'Open.Initiate.Demand.Opinion': 'Open.Demand.Opinion',\n",
    "    'Open.Initiate.Give.Fact': 'Open.Give.Fact', \n",
    "    'Open.Initiate.Demand.Fact': 'Open.Demand.Fact'\n",
    "}\n",
    "make_change = lambda x: [change[el] if el in change else el for el in x]\n",
    "gold_dialogs.gold_answers = gold_dialogs.gold_answers.apply(make_change)\n",
    "gold_dialogs.possible_answers = gold_dialogs.possible_answers.apply(make_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba758ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from gold standard\n",
    "labels = list(set([el for ell in gold_dialogs.gold_answers for el in ell]))\n",
    "labels = sorted(labels)\n",
    "short_labels = sorted(list(set(cut_labels(labels))))\n",
    "\n",
    "# get label dictionaries for long and short labels\n",
    "longlabel2id = {l: i for i, l in enumerate(labels)}\n",
    "shortlabel2id = {l: i for i, l in enumerate(short_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f011175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numeric labels for good annotation\n",
    "gold_dialogs['long_labels_id_gold'] = gold_dialogs.gold_answers.apply(\n",
    "    lambda x: [longlabel2id[el.replace('\\\\n', '')] for el in x])\n",
    "gold_dialogs['long_labels_id'] = gold_dialogs.possible_answers.apply(\n",
    "    lambda x: [longlabel2id[el.replace('\\\\n', '')] for el in x])\n",
    "gold_dialogs['short_labels_id_gold'] = gold_dialogs.gold_answers.apply(\n",
    "    lambda x: [shortlabel2id[el] for el in set(cut_labels(x))])\n",
    "gold_dialogs['short_labels_id'] = gold_dialogs.possible_answers.apply(\n",
    "    lambda x: [shortlabel2id[el] for el in set(cut_labels(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e98396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numeric labels for crowdsource annotation\n",
    "crowdsource['long_labels_id'] = crowdsource.label.apply(longlabel2id.get)\n",
    "crowdsource['short_labels_id'] = crowdsource.short_label.apply(shortlabel2id.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bba771",
   "metadata": {},
   "source": [
    "## Concatenate different annotators' labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f44d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = {\n",
    "    'dialog_id': [],\n",
    "    'utterance': [],\n",
    "    'long_label_1': [],\n",
    "    'long_label_2': [],\n",
    "    'long_label_3': [],\n",
    "    'short_label_1': [],\n",
    "    'short_label_2': [],\n",
    "    'short_label_3': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "083c068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dia_id in sorted(crowdsource.dialog_id.unique()):\n",
    "    dia_len = crowdsource[crowdsource.dialog_id == dia_id].shape[0] // 3\n",
    "    storage['dialog_id'].extend(crowdsource[crowdsource.dialog_id == dia_id][:dia_len].dialog_id.tolist())\n",
    "    storage['utterance'].extend(crowdsource[crowdsource.dialog_id == dia_id][:dia_len].utterance.tolist())\n",
    "    \n",
    "    storage['long_label_1'].extend(crowdsource[crowdsource.dialog_id == dia_id][:dia_len].long_labels_id.tolist())\n",
    "    storage['short_label_1'].extend(crowdsource[crowdsource.dialog_id == dia_id][:dia_len].short_labels_id.tolist())\n",
    "    \n",
    "    storage['long_label_2'].extend(crowdsource[crowdsource.dialog_id == dia_id][dia_len:dia_len*2].long_labels_id.tolist())\n",
    "    storage['short_label_2'].extend(crowdsource[crowdsource.dialog_id == dia_id][dia_len:dia_len*2].short_labels_id.tolist())\n",
    "    \n",
    "    storage['long_label_3'].extend(crowdsource[crowdsource.dialog_id == dia_id][dia_len*2:].long_labels_id.tolist())\n",
    "    storage['short_label_3'].extend(crowdsource[crowdsource.dialog_id == dia_id][dia_len*2:].short_labels_id.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f30c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>long_label_1</th>\n",
       "      <th>long_label_2</th>\n",
       "      <th>long_label_3</th>\n",
       "      <th>short_label_1</th>\n",
       "      <th>short_label_2</th>\n",
       "      <th>short_label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Can you do push-ups?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Of course I can.</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It's a piece of cake!</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Believe it or not.</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I can do 30 push-ups a minute.</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>9554.0</td>\n",
       "      <td>Where should we go?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>9554.0</td>\n",
       "      <td>Why don't we go down to the pool?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>9554.0</td>\n",
       "      <td>It's not too far from here.</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>9554.0</td>\n",
       "      <td>Fine.</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>9554.0</td>\n",
       "      <td>I'll be ready in a minute.</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dialog_id                          utterance  long_label_1  long_label_2  \\\n",
       "0          1.0               Can you do push-ups?             2             2   \n",
       "1          1.0                   Of course I can.            27            26   \n",
       "2          1.0              It's a piece of cake!            31            30   \n",
       "3          1.0                 Believe it or not.            30            31   \n",
       "4          1.0     I can do 30 push-ups a minute.            30            29   \n",
       "..         ...                                ...           ...           ...   \n",
       "605     9554.0                Where should we go?            13            13   \n",
       "606     9554.0  Why don't we go down to the pool?            13            13   \n",
       "607     9554.0        It's not too far from here.            30            30   \n",
       "608     9554.0                              Fine.            27            27   \n",
       "609     9554.0         I'll be ready in a minute.            30            30   \n",
       "\n",
       "     long_label_3  short_label_1  short_label_2  short_label_3  \n",
       "0               2              2              2              2  \n",
       "1              26             12             12             12  \n",
       "2              30             15             15             15  \n",
       "3              30             15             15             15  \n",
       "4              30             15             15             15  \n",
       "..            ...            ...            ...            ...  \n",
       "605            13              7              7              7  \n",
       "606            13              7              7              7  \n",
       "607            30             15             15             15  \n",
       "608            27             12             12             12  \n",
       "609            30             15             15             15  \n",
       "\n",
       "[610 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_crowdsource = pd.DataFrame(storage)\n",
    "new_crowdsource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4be76e",
   "metadata": {},
   "source": [
    "# Count metrics for crowdsource workers\n",
    "\n",
    "## No voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f78b3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_and_toloka_labels(gold_csv: pd.DataFrame, \n",
    "                               label_array: list,\n",
    "                               if_long_label: bool=True,\n",
    "                               full: bool=False):\n",
    "    \"\"\"Prepare arrays for crowdsource and golden annotation.\n",
    "    Parameters:\n",
    "        gold_csv (pd.DataFrame): DataFrame with golden annotation,\n",
    "        label_array (list): list of unified annotations,\n",
    "        if_long_label (bool): if long or short label observed,\n",
    "        full (bool): if all dialogs are observed\n",
    "    Returns:\n",
    "        target (list): target annotations,\n",
    "        annotated (list): actual annotations\"\"\"\n",
    "    target, annotated = [], []\n",
    "    column_to_iter = 'long_labels_id' if if_long_label else 'short_labels_id'\n",
    "    gold_csv = gold_csv.sort_values(by='dialogue_id', kind='stable')\n",
    "    \n",
    "    if not full:\n",
    "        gold_csv = gold_csv.query('`dialogue_id` not in [0, 5, 8]')\n",
    "    \n",
    "    for i, (_, (possible, gold)) in enumerate(\n",
    "        gold_csv[[column_to_iter, f'{column_to_iter}_gold']].iterrows()):\n",
    "        annotated_label = label_array[i]\n",
    "        annotated.append(annotated_label)\n",
    "        if annotated_label in possible or annotated_label in gold:\n",
    "            target.append(annotated_label)\n",
    "        else:\n",
    "            target.append(gold[0])\n",
    "            \n",
    "    return target, annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b0e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(target: list, predicted: list):\n",
    "    \"\"\"Get accuracy, F1-scores (macro & micro), recall and precision metrics.\"\"\"\n",
    "    accuracy = round(accuracy_score(target, predicted), 2)\n",
    "    f1_macro = round(f1_score(target, predicted, average='macro'), 2)\n",
    "    f1_micro = round(f1_score(target, predicted, average='micro'), 2)\n",
    "    recall = round(recall_score(target, predicted, average='weighted'), 2)\n",
    "    precision = round(precision_score(target, predicted, average='weighted'), 2)\n",
    "    return accuracy, f1_macro, f1_micro, recall, precision\n",
    "\n",
    "\n",
    "def shuffle_dataframe(crowdsource_csv: pd.DataFrame,\n",
    "                      if_long_label: bool):\n",
    "    \"\"\"Specific shuffle for crowdsource dataframes.\n",
    "    Parameters:\n",
    "        crowdsource_csv (pd.DataFrame): dataframe with crowdsource annotation,\n",
    "        if_long_label (bool): if long or short label observed\n",
    "    Returns:\n",
    "        crowdsource_csv (pd.DataFrame): shuffled dataframe with crowdsource annotation\"\"\"\n",
    "    if if_long_label:\n",
    "        to_shuffle = crowdsource_csv[crowdsource_csv.columns[2:5]].to_numpy()\n",
    "    else:\n",
    "        to_shuffle = crowdsource_csv[crowdsource_csv.columns[5:]].to_numpy()\n",
    "    for l in to_shuffle:\n",
    "        np.random.shuffle(l)\n",
    "    if if_long_label:\n",
    "        crowdsource_csv[crowdsource_csv.columns[2:5]] = to_shuffle\n",
    "    else:\n",
    "        crowdsource_csv[crowdsource_csv.columns[5:]] = to_shuffle\n",
    "    return crowdsource_csv\n",
    "\n",
    "\n",
    "def annotator_assessment(gold_csv: pd.DataFrame, \n",
    "                          crowdsource_csv: pd.DataFrame):\n",
    "    \"\"\"Get dataframe with annotators' results. The assessment is caried out 10 times and the data is shuffled each time except for the first one.\n",
    "    Parameters:\n",
    "        gold_csv (pd.DataFrame): dataframe with golden annotation,\n",
    "        crowdsource_csv (pd.DataFrame): dataframe with annotators'data which we assess\n",
    "    Returns:\n",
    "        results (list): collected metrics and parameters of metric count\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    params = [\n",
    "            {'annotator_label': ann_id, \n",
    "             'if_long_label': if_ll} for if_ll in [True, False] for ann_id in [1, 2, 3]\n",
    "    ]\n",
    "    for m in range(10):\n",
    "        for param in params:\n",
    "            label_type = 'long' if param['if_long_label'] else 'short'\n",
    "            annotator_label = param['annotator_label']\n",
    "            if m:\n",
    "                crowdsource_csv = shuffle_dataframe(crowdsource_csv, param['if_long_label'])\n",
    "                target, annotated = get_gold_and_toloka_labels(gold_csv, \n",
    "                                                               crowdsource_csv[f'{label_type}_label_{annotator_label}'], \n",
    "                                                               param['if_long_label'])\n",
    "                accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "                results.append(\n",
    "                    ('shuffle', param['annotator_label'], label_type, \n",
    "                     accuracy, f1_macro, f1_micro, recall, precision)\n",
    "                )\n",
    "            else:\n",
    "                target, annotated = get_gold_and_toloka_labels(gold_csv, \n",
    "                                                               crowdsource_csv[f'{label_type}_label_{annotator_label}'], \n",
    "                                                               param['if_long_label'])\n",
    "                accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "                results.append(\n",
    "                    ('no_shuffle', param['annotator_label'], label_type, \n",
    "                     accuracy, f1_macro, f1_micro, recall, precision)\n",
    "                )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd3510d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment(gold_dialogs,\n",
    "                                new_crowdsource)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'shuffle', 'annotator', 'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('metrics_without_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a6e845b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.63       0.02     0.47       0.03     0.63       0.02   \n",
       "2      short     0.82       0.01     0.58       0.03     0.82       0.01   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.63     0.02      0.73        0.01  \n",
       "2   0.82     0.01      0.84        0.01  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('metrics_without_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511f9de-f015-4f8e-855e-024ed3ffe7f5",
   "metadata": {},
   "source": [
    "## With voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a77804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote(ls: list):\n",
    "    \"\"\"Voting fuction that returns mode of annotators' labels or random label if there is no mode\"\"\"\n",
    "    for l in ls:\n",
    "        el1, el2, el3 = l\n",
    "        if el1 != el2 != el3:\n",
    "            yield random.choice(l)\n",
    "        else:\n",
    "            yield mode(l)\n",
    "\n",
    "\n",
    "def annotator_assessment_voting(gold_csv: pd.DataFrame,\n",
    "                                 crowdsource_csv: pd.DataFrame):\n",
    "    \"\"\"Get dataframe with annotators' results. The assessment is caried out 10 times. Voting technique is used for label aggregation.\n",
    "    Parameters:\n",
    "        gold_csv (pd.DataFrame): dataframe with golden annotation,\n",
    "        crowdsource_csv (pd.DataFrame): dataframe with annotators'data which we assess\n",
    "    Returns:\n",
    "        results (list): collected metrics and parameters of metric count\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        labels_long = [el for el in vote(crowdsource_csv[crowdsource_csv.columns[2:5]].values.tolist())]\n",
    "        labels_short = [el for el in vote(crowdsource_csv[crowdsource_csv.columns[5:]].values.tolist())]\n",
    "        target, annotated = get_gold_and_toloka_labels(\n",
    "            gold_csv, labels_long, True\n",
    "        )\n",
    "        accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "        results.append(('long', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "        target, annotated = get_gold_and_toloka_labels(\n",
    "            gold_csv, labels_short, False\n",
    "        )\n",
    "        accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "        results.append(('short', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63cedb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_voting(gold_dialogs,\n",
    "                                new_crowdsource)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('metrics_with_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27cbca30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.65       0.01     0.49       0.01     0.65       0.01   \n",
       "2      short     0.86       0.01     0.61       0.02     0.86       0.01   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.65     0.01      0.75        0.01  \n",
       "2   0.86     0.01      0.86        0.01  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('metrics_with_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf702abf",
   "metadata": {},
   "source": [
    "## Fleiss' kappa\n",
    "\n",
    "|measurement case        | fleiss cappa score |\n",
    "|------------------------|--------------------|\n",
    "|long label crowdsource  | 0.380              |\n",
    "|short label crowdsource | 0.673              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a8d2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_table = aggregate_raters(new_crowdsource[new_crowdsource.columns[2:5]])\n",
    "round(fleiss_kappa(f_table[0], method='fleiss'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ea7b462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_table = aggregate_raters(new_crowdsource[new_crowdsource.columns[5:]])\n",
    "round(fleiss_kappa(f_table[0], method='fleiss'), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b220f-ec6f-4762-b2dc-80b83fcd72c0",
   "metadata": {},
   "source": [
    "# Load LLM annotations\n",
    "\n",
    "### claude-3-haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01fc6f5-2fe7-498f-ae70-a3df9b34e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = pd.read_csv('claude_t0_context1_masking.tsv', sep='\\t')\n",
    "\n",
    "# add missing numeric label \n",
    "longlabel2id['Sustain.Continue.Command'] = 32\n",
    "shortlabel2id['Sustain.Continue.Command'] = 16\n",
    "\n",
    "# add numeric label columns and short label column\n",
    "claude['long_labels_id'] = claude.annotation.apply(longlabel2id.get)\n",
    "claude['short_labels'] = claude.annotation.apply(lambda x: cut_labels([x])[0])\n",
    "claude['short_labels_id'] = claude.short_labels.apply(shortlabel2id.get)\n",
    "\n",
    "claude = claude.sort_values(by='dialog_id', kind='stable').query('`dialog_id` not in [0, 5, 8]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d08c75-58d6-45bf-ad21-5a7d994a9a32",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2319476-7d2d-4cd1-9041-6ae68ce8236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = pd.DataFrame() \n",
    "\n",
    "for filename in ['sf_annotation__chatgpt_t9_masking1.tsv', 'sf_annotation__chatgpt_t9_masking2.tsv',\n",
    "                'sf_annotation__chatgpt_t9_masking3.tsv']:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    chatgpt = pd.concat([chatgpt, df])\n",
    "\n",
    "chatgpt = chatgpt.drop(columns=chatgpt.columns[0])\n",
    "\n",
    "# add numeric label columns and short label column\n",
    "chatgpt['long_labels_id'] = chatgpt.annotation.apply(longlabel2id.get)\n",
    "chatgpt['short_labels'] = chatgpt.annotation.apply(lambda x: cut_labels([x])[0])\n",
    "chatgpt['short_labels_id'] = chatgpt.short_labels.apply(shortlabel2id.get)\n",
    "\n",
    "chatgpt = chatgpt.sort_values(by='dialog_id', kind='stable').query('`dialog_id` not in [0, 5, 8]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89af31-4e25-4b90-baf6-9fd72522bfa9",
   "metadata": {},
   "source": [
    "### Mixtral 8x22B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc505ac-f79c-43a5-af42-53c727ecc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = pd.DataFrame() \n",
    "\n",
    "for filename in ['sf_annotation__mistral_t5_no_masking.tsv', 'full__mistral_t5_no_masking.tsv']:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    mistral = pd.concat([mistral, df])\n",
    "\n",
    "mistral = mistral.drop(columns=mistral.columns[0])\n",
    "mistral = mistral.replace({'React.Rejoinder.Confront.Response.Re-challenge-React.Rejoinder.Confront.Response.Re-challenge':\n",
    "                          'React.Rejoinder.Confront.Response.Re-challenge'})\n",
    "\n",
    "# add missing numeric label \n",
    "longlabel2id['React.Respond.Command'] = 33\n",
    "shortlabel2id['React.Respond.Command'] = 17\n",
    "\n",
    "# add numeric label columns and short label column\n",
    "mistral['long_labels_id'] = mistral.annotation.apply(longlabel2id.get)\n",
    "mistral['short_labels'] = mistral.annotation.apply(lambda x: cut_labels([x])[0])\n",
    "mistral['short_labels_id'] = mistral.short_labels.apply(shortlabel2id.get)\n",
    "\n",
    "mistral = mistral.sort_values(by='dialog_id', kind='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d026c1-79dc-40e8-9a53-ade129a66118",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dialogs = gold_dialogs.sort_values(by='dialogue_id', kind='stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f082c",
   "metadata": {},
   "source": [
    "# Hybrid annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c7f1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_labels(human: list, ai: list):\n",
    "    \"\"\"Get hybrid label arrays.\n",
    "    Parameters:\n",
    "        human (list): list of lists with human annotations,\n",
    "        ai (list): list with LLM annotations\n",
    "    Yields:\n",
    "        list of 2 human annotations and 1 LLM annotation\"\"\"\n",
    "    for i, l in enumerate(human):\n",
    "        random.shuffle(l)\n",
    "        yield l[:2] + [ai[i]]\n",
    "\n",
    "\n",
    "def annotator_assessment_hybrid(gold_csv: pd.DataFrame, \n",
    "                                 crowdsource_csv: pd.DataFrame,\n",
    "                                 ai_csv: pd.DataFrame):\n",
    "    \"\"\"Get dataframe with hybrid annotation results. \n",
    "    The assessment is caried out 30 times and the hybrid annotation data is obtained each 3 time.\n",
    "    No voting is used.\n",
    "    Parameters:\n",
    "        gold_csv (pd.DataFrame): dataframe with golden annotation,\n",
    "        crowdsource_csv (pd.DataFrame): dataframe with annotators'data which we assess,\n",
    "        ai_csv (pd.DataFrame): dataframe with LLM annotation\n",
    "    Returns:\n",
    "        results (list): collected metrics and parameters of metric count\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    ai_csv = ai_csv.sort_values(by='dialog_id', kind='stable').query('`dialog_id` not in [0, 5, 8]')\n",
    "\n",
    "    for m in range(10):\n",
    "        \n",
    "        hybrid_labels_long = list(get_hybrid_labels(\n",
    "            crowdsource_csv[crowdsource_csv.columns[2:5]].values.tolist(),\n",
    "            ai_csv['long_labels_id'].values.tolist()\n",
    "        ))\n",
    "        hybrid_labels_short = list(get_hybrid_labels(\n",
    "            crowdsource_csv[crowdsource_csv.columns[5:]].values.tolist(),\n",
    "            ai_csv['short_labels_id'].values.tolist()\n",
    "        ))\n",
    "        for i in range(3):\n",
    "            target, annotated = get_gold_and_toloka_labels(gold_csv, \n",
    "                                                           hybrid_labels_long)\n",
    "            annotated1, annotated2, annotated3 = [], [], []\n",
    "            \n",
    "            for el1, el2, el3 in annotated:\n",
    "                annotated1.append(el1)\n",
    "                annotated2.append(el2)\n",
    "                annotated3.append(el3)\n",
    "                \n",
    "            for annotated in [annotated1, annotated2, annotated3]:\n",
    "                accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "                results.append(('long', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "                \n",
    "            target, annotated = get_gold_and_toloka_labels(gold_csv, \n",
    "                                                           hybrid_labels_short, \n",
    "                                                           False)\n",
    "            annotated1, annotated2, annotated3 = [], [], []\n",
    "            \n",
    "            for el1, el2, el3 in annotated:\n",
    "                annotated1.append(el1)\n",
    "                annotated2.append(el2)\n",
    "                annotated3.append(el3)\n",
    "                \n",
    "            for annotated in [annotated1, annotated2, annotated3]:    \n",
    "                accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "                results.append(('short', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9225a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotator_assessment_voting_hybrid(gold_csv: pd.DataFrame, \n",
    "                                 crowdsource_csv: pd.DataFrame,\n",
    "                                 ai_csv: pd.DataFrame):\n",
    "    \"\"\"Get dataframe with hybrid annotation results. \n",
    "    The assessment is caried out 10 times. Voting is used.\n",
    "    Parameters:\n",
    "        gold_csv (pd.DataFrame): dataframe with golden annotation,\n",
    "        crowdsource_csv (pd.DataFrame): dataframe with annotators'data which we assess,\n",
    "        ai_csv (pd.DataFrame): dataframe with LLM annotation\n",
    "    Returns:\n",
    "        results (list): collected metrics and parameters of metric count\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    ai_csv = ai_csv.sort_values(by='dialog_id', kind='stable').query('`dialog_id` not in [0, 5, 8]')\n",
    "\n",
    "    for m in range(10):\n",
    "        \n",
    "        hybrid_labels_long = list(get_hybrid_labels(\n",
    "            crowdsource_csv[crowdsource_csv.columns[2:5]].values.tolist(),\n",
    "            ai_csv['long_labels_id'].values.tolist()\n",
    "        ))\n",
    "        hybrid_labels_short = list(get_hybrid_labels(\n",
    "            crowdsource_csv[crowdsource_csv.columns[5:]].values.tolist(),\n",
    "            ai_csv['short_labels_id'].values.tolist()\n",
    "        ))\n",
    "    \n",
    "        labels_long = [el for el in vote(hybrid_labels_long)]\n",
    "        labels_short = [el for el in vote(hybrid_labels_short)]\n",
    "        target, annotated = get_gold_and_toloka_labels(\n",
    "            gold_csv, labels_long, True\n",
    "        )\n",
    "        accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "        results.append(('long', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "        target, annotated = get_gold_and_toloka_labels(\n",
    "            gold_csv, labels_short, False\n",
    "        )\n",
    "        accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "        results.append(('short', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f407a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa_results(crowdsource_csv: pd.DataFrame,\n",
    "                         ai_csv: pd.DataFrame):\n",
    "    \"\"\"Get Fleiss' kappa results for hybrid annotation. The assessment is caried out 100 times.\n",
    "    Parameters:\n",
    "        crowdsource_csv (pd.DataFrame): dataframe with annotators'data which we assess,\n",
    "        ai_csv (pd.DataFrame): dataframe with LLM annotation\n",
    "    Returns:\n",
    "        results (list): collected Fleiss' kappa\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        hybrid_labels_long = list(get_hybrid_labels(\n",
    "            crowdsource_csv[crowdsource_csv.columns[2:5]].values.tolist(),\n",
    "            ai_csv['long_labels_id'].values.tolist()\n",
    "        ))\n",
    "        hybrid_labels_short = list(get_hybrid_labels(\n",
    "            crowdsource_csv[crowdsource_csv.columns[5:]].values.tolist(),\n",
    "            ai_csv['short_labels_id'].values.tolist()\n",
    "        ))\n",
    "        f_table = aggregate_raters(hybrid_labels_long)\n",
    "        fk = round(fleiss_kappa(f_table[0], method='fleiss'), 3)\n",
    "        results.append(('long', fk))\n",
    "        f_table = aggregate_raters(hybrid_labels_short)\n",
    "        fk = round(fleiss_kappa(f_table[0], method='fleiss'), 3)\n",
    "        results.append(('short', fk))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd58d6e-7e9b-4b2a-bc54-a659f518a709",
   "metadata": {},
   "source": [
    "### claude-3-haiku metrics\n",
    "**No voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "449c029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_hybrid(gold_dialogs,\n",
    "                                       new_crowdsource,\n",
    "                                       claude)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('hybrid_metrics_claude_without_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c8a8581-69b9-4e94-b2ad-10c85582b4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long      0.4       0.06      0.3       0.04      0.4       0.06   \n",
       "2      short     0.72       0.05     0.45       0.06     0.72       0.05   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1    0.4     0.06      0.57        0.02  \n",
       "2   0.72     0.05      0.77        0.02  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('hybrid_metrics_claude_without_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d50e6c-4b60-4cc0-9b6a-62f5a65dad1d",
   "metadata": {},
   "source": [
    "**Voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ad2bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_voting_hybrid(gold_dialogs,\n",
    "                                       new_crowdsource,\n",
    "                                       claude)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('hybrid_metrics_claude_with_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0463f3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.62       0.01     0.46       0.02     0.62       0.01   \n",
       "2      short     0.82       0.01     0.59       0.04     0.82       0.01   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.62     0.01      0.73        0.01  \n",
       "2   0.82     0.01      0.84        0.01  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('hybrid_metrics_claude_with_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7b999",
   "metadata": {},
   "source": [
    "**Fleiss' kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24aeda79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short</th>\n",
       "      <td>0.580</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean    std\n",
       "label type              \n",
       "long        0.332  0.007\n",
       "short       0.580  0.006"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fleiss_kappa_results(new_crowdsource, claude)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'fleiss kappa'\n",
    "])\n",
    "results.groupby('label type')['fleiss kappa'].aggregate(\n",
    "    ['mean', 'std']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff1e69-fa4c-4794-a1a6-2a323e70f652",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo metrics\n",
    "**No voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d09d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_hybrid(gold_dialogs,\n",
    "                                       new_crowdsource,\n",
    "                                       chatgpt)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('hybrid_metrics_chatgpt_without_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f62159c-5605-4402-90f3-1be0bf95b807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.39       0.06     0.29       0.04     0.39       0.06   \n",
       "2      short     0.74       0.03     0.45       0.05     0.74       0.03   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.39     0.06      0.55        0.02  \n",
       "2   0.74     0.03      0.78        0.01  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('hybrid_metrics_chatgpt_without_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e035f7-eb2f-4e03-918e-b2eb43480f2d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67ac4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_voting_hybrid(gold_dialogs,\n",
    "                                       new_crowdsource,\n",
    "                                       chatgpt)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('hybrid_metrics_chatgpt_with_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c24aec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.61       0.01     0.46       0.02     0.61       0.01   \n",
       "2      short     0.83       0.01     0.63       0.03     0.83       0.01   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.61     0.01      0.73        0.01  \n",
       "2   0.83     0.01      0.86        0.01  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('hybrid_metrics_chatgpt_with_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67055282",
   "metadata": {},
   "source": [
    "**Fleiss' kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5426b29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.314</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short</th>\n",
       "      <td>0.609</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean    std\n",
       "label type              \n",
       "long        0.314  0.007\n",
       "short       0.609  0.008"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fleiss_kappa_results(new_crowdsource, chatgpt)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'fleiss kappa'\n",
    "])\n",
    "results.groupby('label type')['fleiss kappa'].aggregate(\n",
    "    ['mean', 'std']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02707167-7337-465f-ba78-cb7d2db14b95",
   "metadata": {},
   "source": [
    "### Mixtral 8x22B metrics\n",
    "**No voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e508b16-8b89-40f6-acd2-5918a1f5b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_hybrid(gold_dialogs,\n",
    "                                       new_crowdsource,\n",
    "                                       mistral)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('hybrid_metrics_mistral_without_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb029dfc-54fa-4e64-9c99-49a3d411715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.39       0.06     0.29       0.04     0.39       0.06   \n",
       "2      short     0.69       0.11     0.43       0.08     0.69       0.11   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.39     0.06      0.55        0.01  \n",
       "2   0.69     0.11      0.77        0.02  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('hybrid_metrics_mistral_without_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775e97e-67ec-4a4e-a9a2-ed3661e1ee1e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9401c4e1-6c76-4cf5-8f5c-0c5d6bf08e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = annotator_assessment_voting_hybrid(gold_dialogs,\n",
    "                                       new_crowdsource,\n",
    "                                       mistral)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('hybrid_metrics_mistral_with_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af61327a-101f-46b9-85c5-f51317dc177d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.62       0.01     0.43       0.01     0.62       0.01   \n",
       "2      short     0.82       0.01     0.56       0.03     0.82       0.01   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.62     0.01      0.73        0.01  \n",
       "2   0.82     0.01      0.84        0.01  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('hybrid_metrics_mistral_with_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efe90e-e337-4405-a7d8-cae28d090070",
   "metadata": {},
   "source": [
    "**Fleiss' kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bfd23fa-83e5-4c12-8354-c5991b0bc68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.117</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean    std\n",
       "label type              \n",
       "long        0.117  0.005\n",
       "short       0.197  0.004"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fleiss_kappa_results(new_crowdsource, mistral)\n",
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'fleiss kappa'\n",
    "])\n",
    "results.groupby('label type')['fleiss kappa'].aggregate(\n",
    "    ['mean', 'std']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07999376",
   "metadata": {},
   "source": [
    "## Metrics for all LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "660b0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_annotated(gold:list, \n",
    "                         good: list, \n",
    "                         model_annotated: list):\n",
    "    \"\"\"Prepare arrays with golden and LLM annotations\n",
    "    Parameters:\n",
    "        gold (list): list of golden annotations,\n",
    "        good (list): list of possible annotations,\n",
    "        model_annotated (list): list of LLM annotations\n",
    "    Returns:\n",
    "        target (list): target annotations,\n",
    "        annotated (list): actual annotations\"\"\"\n",
    "    target, annotated = [], []\n",
    "    \n",
    "    for i, pred in enumerate(model_annotated):\n",
    "        if pred in gold[i] or pred in good[i]:\n",
    "            target.append(pred)\n",
    "            annotated.append(pred)\n",
    "        else:\n",
    "            target.append(gold[i][0])\n",
    "            annotated.append(pred)\n",
    "            \n",
    "    return target, annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d0362-2be5-45d7-93c5-bb6d1f3e8b18",
   "metadata": {},
   "source": [
    "**No voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "663807a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "models = {'claude': claude, 'chatgpt': chatgpt, 'mistral': mistral}\n",
    "\n",
    "for model in models:\n",
    "    for label_length in ['long', 'short']:\n",
    "        target, annotated = get_target_annotated(gold_dialogs[f'{label_length}_labels_id_gold'].tolist(),\n",
    "                                                 gold_dialogs[f'{label_length}_labels_id'].tolist(),\n",
    "                                                 models[model][f'{label_length}_labels_id'].tolist())\n",
    "        accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "        results.append((label_length, model, accuracy, f1_macro, f1_micro, recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "881a9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'model annotator', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('llm_metrics_without_voting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f983fba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro f1_micro.1  \\\n",
       "0        NaN     mean        std     mean        std     mean        std   \n",
       "1       long     0.49       0.02     0.33       0.02     0.49       0.02   \n",
       "2      short     0.68       0.09     0.44       0.08     0.68       0.09   \n",
       "\n",
       "  recall recall.1 precision precision.1  \n",
       "0   mean      std      mean         std  \n",
       "1   0.49     0.02      0.68        0.03  \n",
       "2   0.68     0.09      0.79        0.02  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('llm_metrics_without_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665aaa8c-b5c1-4e9e-b3f3-66d719f1f9ad",
   "metadata": {},
   "source": [
    "**Voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da6fe9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = list(zip(claude['long_labels_id'].tolist(), \n",
    "         chatgpt['long_labels_id'].tolist(),\n",
    "         mistral['long_labels_id'].tolist(),))\n",
    "short = list(zip(claude['short_labels_id'].tolist(), \n",
    "         chatgpt['short_labels_id'].tolist(),\n",
    "         mistral['short_labels_id'].tolist()))\n",
    "long_voting = list(vote(long))\n",
    "short_voting = list(vote(short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59950640",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _ in range(10):\n",
    "    target, annotated = get_target_annotated(gold_dialogs.long_labels_id_gold.tolist(),\n",
    "                                             gold_dialogs.long_labels_id.tolist(),\n",
    "                                             long_voting)\n",
    "    accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "    results.append(('long', accuracy, f1_macro, f1_micro, recall, precision))\n",
    "    target, annotated = get_target_annotated(gold_dialogs.short_labels_id_gold.tolist(),\n",
    "                                             gold_dialogs.short_labels_id.tolist(),\n",
    "                                             short_voting)\n",
    "    accuracy, f1_macro, f1_micro, recall, precision = metrics(target, annotated)\n",
    "    results.append(('short', accuracy, f1_macro, f1_micro, recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6cb2caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results, columns=[\n",
    "    'label type', 'accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision'\n",
    "])\n",
    "results.groupby('label type')[['accuracy', 'f1_macro', 'f1_micro', 'recall', 'precision']].aggregate(\n",
    "    ['mean', 'std']).round(2).reset_index().to_csv('llm_metrics_with_voting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "412b982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy.1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_macro.1</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_micro.1</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall.1</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>long</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label type accuracy accuracy.1 f1_macro f1_macro.1 f1_micro  \\\n",
       "0         NaN        NaN     mean        std     mean        std     mean   \n",
       "1         0.0       long     0.54        0.0     0.38        0.0     0.54   \n",
       "2         1.0      short     0.74        0.0      0.5        0.0     0.74   \n",
       "\n",
       "  f1_micro.1 recall recall.1 precision precision.1  \n",
       "0        std   mean      std      mean         std  \n",
       "1        0.0   0.54      0.0      0.67         0.0  \n",
       "2        0.0   0.74      0.0      0.79         0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('llm_metrics_with_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d538055",
   "metadata": {},
   "source": [
    "**Fleiss' kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3513681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_table = aggregate_raters(long)\n",
    "f_table_long = round(fleiss_kappa(f_table[0], method='fleiss'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9300b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_table = aggregate_raters(short)\n",
    "f_table_short = round(fleiss_kappa(f_table[0], method='fleiss'), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
